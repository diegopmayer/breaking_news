{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fYRI2yb8R3Qn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from requests import get, post\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "from selenium import webdriver "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get links from google trends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrap_link():\n",
        "\n",
        "    #---------------------------link from google trends daily\n",
        "    url = 'https://trends.google.com.br/trends/trendingsearches/daily?geo=BR'\n",
        "\n",
        "    #---------------------------Make parser and put to the soup\n",
        "    browser = webdriver.Chrome()\n",
        "    browser.get(url)\n",
        "    soup = bs(browser.page_source, 'html.parser')\n",
        "    browser.close()\n",
        "\n",
        "    #---------------------------Get the title and links from google\n",
        "    links, titles = [], []\n",
        "    for getLink in soup.find_all('div', {'ng-if':\"::imageUrl\"}):\n",
        "        title = getLink.find('a').get('title')\n",
        "        linkHref = getLink.find('a').get('href')\n",
        "        links.append(linkHref), titles.append(title)\n",
        "    \n",
        "    #---------------------------return a list of Links and Titles\n",
        "    return titles, links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scraping from page news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrap_page(link, soupN, ind_link):\n",
        "    link, soupN, ind_link = link, soupN, ind_link\n",
        "\n",
        "    \"\"\"Get link from google trends list of links and \n",
        "        scraping page and store in variables\"\"\"\n",
        "\n",
        "    #-----------------------------Condition for chose wicth tag you going to scraping\n",
        "    #-----------------------------Get the domain from url, split slash and get the index\n",
        "    domainN = link.split('/')[2]\n",
        "    #-----------------------------G1\n",
        "    newslist = []\n",
        "    list_domain = ['g1.globo.com', 'tvi24.iol.pt', 'istoe.com.br']\n",
        "    if domainN in list_domain:\n",
        "        if domainN == 'g1.globo.com':\n",
        "            titleN = soupN.find('title').get_text()\n",
        "            imgN = soupN.find('picture').find('img').get('src')\n",
        "            for pcont in soupN.find_all('p', {'class': 'content-text__container'}):\n",
        "                newslist.append(pcont.get_text())\n",
        "\n",
        "        #-----------------------------tvi24.iol.pt\n",
        "        elif domainN == 'tvi24.iol.pt':\n",
        "            titleN = soupN.find('title').get_text()\n",
        "            imgN = soupN.find('figure').find('img').get('src')\n",
        "            for p in soupN.find_all('p'):\n",
        "                newslist.append(pcont.get_text())\n",
        "\n",
        "        #-----------------------------Isto Ã‰\n",
        "        elif domainN == 'istoe.com.br':\n",
        "            titleN = soupN.find('title').get_text()\n",
        "            imgN = soupN.find('div', {'class':\"text_imagem\"}).find('img').get('src')\n",
        "\n",
        "            for p in soupN.find_all('p'):\n",
        "                if p.get_text() != \"\":\n",
        "                    if p.get_text() != ' ':\n",
        "                        newslist.append(p.get_text())\n",
        "\n",
        "        #------------------------------JCNet \n",
        "        elif domainN == 'jcnet.com.br':\n",
        "            titleN = soupN.find('title').get_text()\n",
        "            soupN.find('div', {'class':'content-noticia'}).find(\n",
        "                                'a').find('img').get('src')\n",
        "\n",
        "            for p in soupN.find('div', {'class':'content-noticia'\n",
        "                                        }).find_all('p'):\n",
        "                newslist.(p.get_text())\n",
        "    #------------------------------Get the words and create variables to store the news\n",
        "    else:\n",
        "        titleN = soupN.find('title').get_text()\n",
        "        imgN = soupN.find('figure').find('img').get('src')\n",
        "\n",
        "        for p in soupN.find_all('p'):\n",
        "            newslist.append(p.get_text())\n",
        "\n",
        "    return [ind_link, titleN, imgN, newslist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    titles, links = scrap_link()\n",
        "    newsPage = []\n",
        "    for link in links:\n",
        "\n",
        "        #----------------------------Parser from links page\n",
        "        ind_link = links.index(link)\n",
        "        browsN = webdriver.Chrome()\n",
        "        browsN.get(link)\n",
        "        soupN = bs(browsN.page_source, 'html.parser')\n",
        "        #browsN.close()\n",
        "\n",
        "        newsPage.append(scrap_page(link, soupN, ind_link))\n",
        "    return newsPage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tests Code Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titles, links = scrap_link()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'find'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-228-6cf8b8502c5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdomainN\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'istoe.com.br'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtitleN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoupN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mimgN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoupN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"text_imagem\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoupN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
          ]
        }
      ],
      "source": [
        "newsPage = []\n",
        "link = links[0]\n",
        "newslist = []\n",
        "#----------------------------Parser from links page\n",
        "#ind_link = links.index(link)\n",
        "browsN = webdriver.Chrome()\n",
        "browsN.get(link)\n",
        "soupN = bs(browsN.page_source, 'html.parser')\n",
        "#browsN.close()\n",
        "\n",
        "if domainN == 'istoe.com.br':\n",
        "    titleN = soupN.find('title').get_text()\n",
        "    imgN = soupN.find('div', {'class':\"text_imagem\"}).find('img').get('src')\n",
        "\n",
        "    for p in soupN.find_all('p'):\n",
        "        if p.get_text() != \"\":\n",
        "            if p.get_text() != ' ':\n",
        "                newslist.append(p.get_text())\n",
        "\n",
        "else:\n",
        "    titleN = soupN.find('title').get_text()\n",
        "    imgN = soupN.find('figure').find('img').get('src')\n",
        "\n",
        "    for p in soupN.find_all('p'):\n",
        "        newslist.append(p.get_text())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python37464bitvenvvenv6a9964ecec784b919473c820c436876e",
      "display_name": "Python 3.7.4 64-bit ('venv': venv)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}