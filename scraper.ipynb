{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fYRI2yb8R3Qn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from requests import get, post\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "from selenium import webdriver "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get links from google trends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------------link from google trends daily\n",
        "url = 'https://trends.google.com.br/trends/trendingsearches/daily?geo=BR'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------------Make parser and put to the soup\n",
        "browser = webdriver.Chrome()\n",
        "browser.get(url)\n",
        "soup = bs(browser.page_source, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVExpRUgSaly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "99b99b05-b325-4deb-afc3-1f2fa4e3d18c"
      },
      "source": [
        "#---------------------------Get the title and links from google\n",
        "linkHref = soup.find('div', {'ng-if':\"::imageUrl\"}).find('a').get('href')\n",
        "titles = soup.find('div', {'ng-if':\"::imageUrl\"}).find('a').get('title')\n",
        "links = []\n",
        "links.append(linkHref)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scraping from page news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrap_page(link, soupN):\n",
        "\n",
        "    \"\"\"Get link from google trends list of links and \n",
        "        scraping page and store in variables\"\"\"\n",
        "\n",
        "    #---------------------Condition for chose wicth tag you going to scraping\n",
        "    #----------------------------Get the domain from url, split slash and get the index\n",
        "    domainN = link.split('/')[2]\n",
        "        #--------------------------G1\n",
        "    newslist = []\n",
        "    if domainN == 'g1.globo.com':\n",
        "        titleN = soupN.find('title').get_text()\n",
        "        imgN = soupN.find('picture').find('img').get('src')\n",
        "        for pcont in soupN.find_all('p', {'class': 'content-text__container'}):\n",
        "            newslist.append(pcont.get_text())\n",
        "\n",
        "        #--------------------------tvi24.iol.pt\n",
        "    elif domainN == 'tvi24.iol.pt':\n",
        "        titleN = soupN.find('title').get_text()\n",
        "        imgN = soupN.find('figure').find('img').get('src')\n",
        "        for p in soupN.find_all('p'):\n",
        "            newslist.append(pcont.get_text())\n",
        "\n",
        "    else:\n",
        "        #----------------------------Get the words and create variables to store the news\n",
        "        titleN = soupN.find('title').get_text()\n",
        "        for p in soupN.find_all('p'):\n",
        "            newslist.append(p.get_text())\n",
        "    return titleN, imgN, newslist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'imgN' referenced before assignment",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-112-f33f3dda3782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnewsPage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mnewsPage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscrap_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoupN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-111-6feb67cfac61>\u001b[0m in \u001b[0;36mscrap_page\u001b[1;34m(link, soupN)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoupN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mnewslist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtitleN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewslist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'imgN' referenced before assignment"
          ]
        }
      ],
      "source": [
        "for link in links:\n",
        "    #----------------------------Parser from links page\n",
        "    browsN = webdriver.Chrome()\n",
        "    browsN.get(link)\n",
        "    soupN = bs(browsN.page_source, 'html.parser')\n",
        "\n",
        "    newsPage = []\n",
        "    newsPage.append(scrap_page(link, soupN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python37464bitvenvvenv6a9964ecec784b919473c820c436876e",
      "display_name": "Python 3.7.4 64-bit ('venv': venv)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}